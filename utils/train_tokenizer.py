import os
from glob import glob
import sentencepiece as spm

def train_tokenizer_from_txt_folder(txt_folder, model_prefix='utils/tokenizer800', vocab_size=1300):
    # 1. ëª¨ë“  .txt íŒŒì¼ ê²½ë¡œ ìˆ˜ì§‘
    txt_files = glob(os.path.join(txt_folder, "*.txt"))
    if len(txt_files) == 0:
        raise ValueError(f"âŒ {txt_folder} í´ë”ì— .txt íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")

    print(f"ğŸ“‚ {len(txt_files)}ê°œì˜ ìë§‰ íŒŒì¼ì—ì„œ í† í¬ë‚˜ì´ì € í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")

    # 2. ì‰¼í‘œë¡œ ê²½ë¡œ ì—°ê²°
    input_files = ",".join(txt_files)

    # 3. SentencePiece tokenizer í•™ìŠµ
    spm.SentencePieceTrainer.train(
        input=input_files,
        model_prefix=model_prefix,
        vocab_size=vocab_size,
        model_type='unigram',
        character_coverage=1.0,
        user_defined_symbols=['<blank>']
    )

    print(f"âœ… í•™ìŠµ ì™„ë£Œ: {model_prefix}.model / {model_prefix}.vocab")

# train_tokenizer_from_txt_folder("processed_dataset/text")
