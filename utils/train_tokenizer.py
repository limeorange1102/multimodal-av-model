import os
from glob import glob
import sentencepiece as spm

def train_tokenizer_from_txt_folder(txt_folder, model_prefix='utils/tokenizer800', vocab_size=1300):
    txt_files = glob(os.path.join(txt_folder, "*.txt"))
    if len(txt_files) == 0:
        raise ValueError(f"âŒ {txt_folder} í´ë”ì— .txt íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")

    print(f"ğŸ“‚ {len(txt_files)}ê°œì˜ ìë§‰ íŒŒì¼ì—ì„œ í† í¬ë‚˜ì´ì € í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")

    input_files = ",".join(txt_files)

    spm.SentencePieceTrainer.train(
        input=input_files,
        model_prefix=model_prefix,
        vocab_size=vocab_size,
        model_type='char',  # âœ… ê¸€ì ë‹¨ìœ„
        character_coverage=1.0,
        user_defined_symbols=['<blank>', ' ']  # âœ… ê³µë°± í¬í•¨
    )

    print(f"âœ… í•™ìŠµ ì™„ë£Œ: {model_prefix}.model / {model_prefix}.vocab")


train_tokenizer_from_txt_folder("processed_dataset/text")
