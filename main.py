import torch
from torch.utils.data import DataLoader
import os, random, numpy as np
from sklearn.model_selection import train_test_split

from dataset.multi_speaker_dataset import RandomSentencePairDataset, FixedSentencePairDataset
from dataset.collate_fn import collate_fn
from model.encoder import PositionalEncoding, RivaConformerAudioEncoder
from model.fusion_module import CrossAttentionFusion
from model.decoder import CTCDecoder
from model.trainer import MultimodalTrainer
from utils.tokenizer import Tokenizer
from preprocessing import build_data_list

import logging

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)

def set_seed(seed=42):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)

def generate_fixed_pairs(sentence_list, n_pairs=1000):
    pairs = []
    indices = list(range(len(sentence_list)))
    for _ in range(n_pairs):
        i, j = random.sample(indices, 2)
        pairs.append((sentence_list[i], sentence_list[j]))
    return pairs

# âœ… ì²´í¬í¬ì¸íŠ¸ ì €ì¥
def save_checkpoint(epoch, trainer, path):
    torch.save({
        'epoch': epoch,
        'visual_encoder': trainer.visual_encoder.state_dict(),
        'audio_encoder': trainer.audio_encoder.state_dict(),
        'fusion': trainer.fusion_module.state_dict(),
        'decoder': trainer.decoder.state_dict(),
        'optimizer': trainer.optimizer.state_dict(),
    }, path)

# âœ… ì²´í¬í¬ì¸íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°
def load_checkpoint(trainer, path):
    checkpoint = torch.load(path, map_location=trainer.device)
    trainer.visual_encoder.load_state_dict(checkpoint['visual_encoder'])
    trainer.audio_encoder.load_state_dict(checkpoint['audio_encoder'])
    trainer.fusion_module.load_state_dict(checkpoint['fusion'])
    trainer.decoder.load_state_dict(checkpoint['decoder'])
    trainer.optimizer.load_state_dict(checkpoint['optimizer'])
    return checkpoint['epoch'] + 1  # ë‹¤ìŒ epochë¶€í„° ì‹œì‘

def main():
    set_seed()

    # âœ… ê²½ë¡œ ì„¤ì •
    json_folder = "input_texts"
    npy_dir = "processed_dataset/npy"
    text_dir = "processed_dataset/text"
    wav_dir = "input_videos"

    tokenizer = Tokenizer(vocab_path="utils/tokenizer800.vocab")
    sentence_list = build_data_list(json_folder, npy_dir, text_dir, wav_dir)
    train_sent, val_sent = train_test_split(sentence_list, test_size=0.1, random_state=42)
    val_pairs = generate_fixed_pairs(val_sent, n_pairs=500)

    train_dataset = RandomSentencePairDataset(train_sent, tokenizer, num_pairs_per_epoch=10000)
    val_dataset = FixedSentencePairDataset(val_pairs, tokenizer)

    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4, collate_fn=collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=4, collate_fn=collate_fn)

    # âœ… ëª¨ë¸ êµ¬ì„±
    visual_encoder = PositionalEncoding(
        hidden_dim=256, lstm_layers=2, bidirectional=True
    )
    audio_encoder = RivaConformerAudioEncoder(freeze=False)
    fusion = CrossAttentionFusion(
        visual_dim=visual_encoder.output_dim,
        audio_dim=audio_encoder.output_dim,
        fused_dim=512
    )
    decoder = CTCDecoder(input_dim=512, vocab_size=tokenizer.vocab_size, blank_id=tokenizer.blank_id)

    trainer = MultimodalTrainer(
        visual_encoder, audio_encoder, fusion, decoder,
        tokenizer,
        learning_rate=1e-4,
        device="cuda" if torch.cuda.is_available() else "cpu"
    )

    # âœ… ì²´í¬í¬ì¸íŠ¸ í´ë” ë° ê²½ë¡œ
    os.makedirs("checkpoints", exist_ok=True)
    last_ckpt_path = "checkpoints/last_checkpoint.pt"
    best_ckpt_path = "checkpoints/best_checkpoint.pt"
    start_epoch = 1
    best_wer = 1.0  # ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ

    # âœ… ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ê°€ ìˆìœ¼ë©´ ì´ì–´ì„œ ì‹œì‘
    if os.path.exists(last_ckpt_path):
        logging.info("ğŸ” ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
        start_epoch = load_checkpoint(trainer, last_ckpt_path)
        logging.info(f"â¡ï¸  Epoch {start_epoch}ë¶€í„° ì¬ê°œ")

    # âœ… í•™ìŠµ ë£¨í”„
    for epoch in range(start_epoch, 21):
        logging.info(f"\nğŸ“š Epoch {epoch}/20")
        loss = trainer.train_epoch(train_loader)

        wer_score = trainer.evaluate(val_loader)

        # ğŸ’¾ ë§ˆì§€ë§‰ ìƒíƒœ ì €ì¥
        save_checkpoint(epoch, trainer, last_ckpt_path)
        logging.info("ğŸ’¾ ë§ˆì§€ë§‰ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì™„ë£Œ")

        # ğŸ… Best ì„±ëŠ¥ ëª¨ë¸ ë”°ë¡œ ì €ì¥
        if wer_score < best_wer:
            best_wer = wer_score
            save_checkpoint(epoch, trainer, best_ckpt_path)
            logging.info("ğŸ… Best ëª¨ë¸ ê°±ì‹  ë° ì €ì¥ ì™„ë£Œ")

if __name__ == "__main__":
    main()
